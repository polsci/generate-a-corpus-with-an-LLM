{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d7db3e",
   "metadata": {},
   "source": [
    "# DIGI405 - Generate a corpus with an LLM\n",
    "\n",
    "Geoff Ford  \n",
    "[https://geoffford.nz](https://geoffford.nz/)  \n",
    "\n",
    "There are five examples in this notebook. Work through them and think about the relevance for your assignment.  \n",
    "\n",
    "The last cells in the notebook demonstrates how to delete a corpus or zip a corpus for you to download.\n",
    "\n",
    "Consult the [README](README.md) for detailed information and the [CHANGELOG](CHANGELOG.md) for changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336dff11",
   "metadata": {},
   "source": [
    "Run this cell to install required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c330702",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0a100",
   "metadata": {},
   "source": [
    "Run the following cell to import relevant Python libraries used in this notebook and set the logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87380bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import getpass\n",
    "from slugify import slugify\n",
    "import shutil\n",
    "import os\n",
    "import csv\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a89c27",
   "metadata": {},
   "source": [
    "The [README](README.md) file discusses how to generate an OpenRouter API key. Configure the key by running this cell ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572823a2",
   "metadata": {},
   "source": [
    "The following cell contains a function to query Open Router and generate LLM text. Just run it to make the function available. Change it if you know what you are doing.\n",
    "\n",
    "Note: you can use API endpoints compatible with the OpenAI completion endpoint, but you will need to specify the relevant `api_url` and specify an `api_url` to `query_llm` calls. For example, if you have software to run LLMs locally, like [Ollama](https://ollama.com/), you can specifying an `api_url` (e.g. `http://127.0.0.1:11434/v1/chat/completions`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bfb103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(prompt:str, # prompt to send to LLM\n",
    "            model: str, # model name e.g. google/gemma-2-9b-it:free\n",
    "            system_prompt: str = None, # system prompt to send to LLM\n",
    "            max_tokens: int = 2048, # maximum number of tokens to generate (includes prompt tokens)\n",
    "            response_format: str = None, # response format: json or None\n",
    "            temperature: float = None, # temperature for sampling\n",
    "            api_url: str = None # OpenAI completion endpoint compatible API to query, defaults to OpenRouter's API \n",
    "            ) -> str: # generated text from LLM call\n",
    "    \"\"\" Query LLM with prompt \"\"\"\n",
    "\n",
    "    if api_url is None or api_url.strip() == '':\n",
    "        api_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "    \n",
    "    if OPENROUTER_API_KEY is None:\n",
    "        logging.error(\"OPENROUTER_API_KEY not set. Not querying llm.\")\n",
    "        return None\n",
    "    api_key = OPENROUTER_API_KEY\n",
    "    \n",
    "    if prompt.strip() == '':\n",
    "        logging.error('No prompt provided. Not querying llm.')\n",
    "        return None\n",
    "    \n",
    "    messages = []\n",
    "    if system_prompt is not None and system_prompt.strip() != '':\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    request_data = {\n",
    "                \"model\": model, \n",
    "                \"messages\": messages,\n",
    "                'max_tokens': max_tokens\n",
    "                    }\n",
    "\n",
    "    if temperature is not None:\n",
    "        request_data['temperature'] = temperature\n",
    "    \n",
    "    if response_format == \"json\":\n",
    "        request_data['response_format'] = {\"type\": \"json_object\"}\n",
    "        \n",
    "    text = None\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url=api_url,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {api_key}\",\n",
    "            },\n",
    "            data=json.dumps(request_data)\n",
    "            )\n",
    "        response.raise_for_status() \n",
    "        text = response.json()['choices'][0]['message']['content']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error querying LLM: {e}\")\n",
    "        print(response.json())\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"Error querying LLM: {e}\")\n",
    "        print(response.json())\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error querying LLM: {e}\")\n",
    "        print(response.json())\n",
    "        raise\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619145b",
   "metadata": {},
   "source": [
    "## Set the model (and a note about OpenRouter free models)\n",
    "\n",
    "When using OpenRouter's free models it is possible that specific models will be unavailable at times. If you get errors querying OpenRouter you can look up the [message or error codes in their documentation](https://openrouter.ai/docs). The `query_llm` function will raise errors when the API responds with an error code or if the JSON data returned by the API does not include generated content. If you get an error when using a free model, it is likely that this is temporary. Changing to [another free model](https://openrouter.ai/models?max_price=0) will typically resolve the issue. Look for models with '(free)' in their name. The next cell is where you can set the model to use for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2237b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'meta-llama/llama-3-8b-instruct:free'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb64994",
   "metadata": {},
   "source": [
    "## Example 1 - generate a corpus with one LLM prompt\n",
    "\n",
    "This is a very basic example that uses one single prompt to generate a small corpus. Example 2 and 3 will probably be more helpful for your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4de5b0",
   "metadata": {},
   "source": [
    "Set the path to save the corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'example1-corpus/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9da603",
   "metadata": {},
   "source": [
    "Check if the path exists, if not create it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f0e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(corpus_path):\n",
    "    print(f'Creating path: {corpus_path}')\n",
    "    os.makedirs(corpus_path)\n",
    "else:\n",
    "    print(f'Path already exists: {corpus_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae861b",
   "metadata": {},
   "source": [
    "Below are the settings we will use to generate a tiny corpus of three documents. \n",
    "\n",
    "### Note about the temperature setting\n",
    "\n",
    "The only parameter implemented in the `query_llm` function is `temperature` (([video](https://www.youtube.com/watch?v=ezgqHnWvua8)). Feel free to implement [other parameters available in OpenRouter's API](https://openrouter.ai/docs/parameters) if you are confident doing this, but this is not expected for class activities or supported.  \n",
    "\n",
    "Lower temperature values give similiar or identical responses. Higher values produce more varied responses.\n",
    "The default value of temperature is 1.0. It can vary between 0.0 and 2.0.\n",
    "\n",
    "### Warning\n",
    "\n",
    "You can change the number of texts to generate, but DON'T do this during the lab times as this may affect the classes ability to run the notebook. Remember there are limits (200 requests for free models per day, and a maximum of 20 requests per minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e725fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_texts_to_generate = 3 # PLEASE LEAVE THIS FOR NOW!\n",
    "\n",
    "system_prompt = '''\n",
    "'''\n",
    "\n",
    "prompt = '''\n",
    "Write a short children's story imagining the future with AI.\n",
    "'''\n",
    "\n",
    "max_tokens = 1024\n",
    "\n",
    "temperature = 1.5\n",
    "\n",
    "response_format = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9de68",
   "metadata": {},
   "source": [
    "Running the next cell queries the API and generates text. A preview of each generated text is displayed and the generated text is saved as a .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa03a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_texts_to_generate):\n",
    "    response = query_llm(prompt, model, system_prompt, max_tokens, response_format = response_format, temperature = temperature)\n",
    "    print(f'Text {i} preview: {response[0:200]} ...')\n",
    "    \n",
    "    filename = f'text-{i}.txt'\n",
    "    with open(os.path.join(corpus_path, filename), 'w', encoding='utf8') as f:\n",
    "        print(f'Saving to {os.path.join(corpus_path, filename)}')\n",
    "        f.write(response)\n",
    "    \n",
    "    print('------')\n",
    "        \n",
    "    time.sleep(10) # always leave a delay!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe97679",
   "metadata": {},
   "source": [
    "Inspect your txt files in the corpus path you specified above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b7120",
   "metadata": {},
   "source": [
    "## Example 2: generate a corpus by seeding the prompt with some other generated data\n",
    "\n",
    "In this example we first generate some data, and then we use this data to prompt the LLM. For this example the generated data is simply a title, but this could be more complex. For example, if you wanted to generate biographies with specific information, you could generate the required information (e.g. names of people, their life histories). If you wanted to create texts on a topic with very different points of view, you could generate a number of structured personas (e.g. name, where they live, job title, political allegiance) and then use this information as input to produce a variety of different perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a3a8f",
   "metadata": {},
   "source": [
    "Set the path to save the corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d50882",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'example2-corpus/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00573f5a",
   "metadata": {},
   "source": [
    "Check if the path exists, if not create it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813bcff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(corpus_path):\n",
    "    print(f'Creating path: {corpus_path}')\n",
    "    os.makedirs(corpus_path)\n",
    "else:\n",
    "    print(f'Path already exists: {corpus_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e00822a",
   "metadata": {},
   "source": [
    "First, we are going to generate some data that we will then use to generate the corpus. In this case we are going to generate some titles of opinion pieces on AI. Note: here I am generating JSON data. Note: there is a system prompt advising JSON output is required and the user prompt includes the required format. This sometimes may not work with smaller models! If it doesn't, just run it again. If you wanted to generate more titles you would change the number in the user prompt. For now, leave it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd076276",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "Always respond with JSON data.\n",
    "'''\n",
    "\n",
    "prompt = '''\n",
    "Generate a list of 3 editorial titles about artificial intelligence. \n",
    "Vary the length and theme of each title.\n",
    "JSON format: \n",
    "{\n",
    "    \"titles\": [\n",
    "        \"title 1\",\n",
    "        \"title 2\",\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "\n",
    "max_tokens = 4098\n",
    "\n",
    "temperature = 1.0 \n",
    "\n",
    "response_format = 'json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a17cdd",
   "metadata": {},
   "source": [
    "Query the API to generate the titles ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_llm(prompt, model, system_prompt, max_tokens, response_format = response_format, temperature = temperature)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae467915",
   "metadata": {},
   "source": [
    "If the next cell runs and outputs the titles you have some valid JSON. If not, run the cell above again. Sometimes LLMs don't follow output instructions! If you were generating different structured data, you may have to modify this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    seed_data = json.loads(response)\n",
    "    for seed in seed_data['titles']:\n",
    "        print(seed)\n",
    "except json.JSONDecodeError as e:\n",
    "    print('Error decoding JSON. Try regenerating. A lower temperature value may help.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc50ba7",
   "metadata": {},
   "source": [
    "Now generate your tiny corpus of three documents based on the generated title data.  \n",
    "Note: I have specifyied a system prompt, but there is no user prompt specified. The prompt for each story is the title we have generated above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d65e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "Write an editorial based on the title provided. The editorial should be written for a general audience.\n",
    "It will appear in a major news outlet. Do not include the title as part of the output.\n",
    "'''\n",
    "\n",
    "max_tokens = 2048\n",
    "\n",
    "temperature = 1.0 \n",
    "\n",
    "response_format = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888096bf",
   "metadata": {},
   "source": [
    "Run this to generate the corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303753ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in seed_data['titles']:\n",
    "    print(f'Generating text based on: {prompt}')\n",
    "    response = query_llm(prompt, model, system_prompt, max_tokens, response_format = response_format, temperature = temperature)\n",
    "    print(f'Text {i} preview: {response[0:200]} ...')\n",
    "\n",
    "    filename = slugify(prompt, max_length=25) + '.txt' # Note: this creates a nice filename from the title\n",
    "    with open(os.path.join(corpus_path, filename), 'w', encoding='utf8') as f:\n",
    "        print(f'Saving to {os.path.join(corpus_path, filename)}')\n",
    "        f.write(response)\n",
    "    \n",
    "    print('-----------------')\n",
    "    time.sleep(10) # always use a time delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17b34b",
   "metadata": {},
   "source": [
    "Inspect your txt files in the corpus path you specified above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c8760",
   "metadata": {},
   "source": [
    "## Example 3: generate a corpus by with a prompt from a CSV file\n",
    "\n",
    "If you are using another data source, whether that is scraped or a corpus you have found online, you may want to generate  comparable corpus using this data. For example, if you want to compare human vs generated news stories and have the human-authored stories collected, you could generate texts based on the titles of the human-authored texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856c6dd",
   "metadata": {},
   "source": [
    "Set the path to save the corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'example3-corpus/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7206a",
   "metadata": {},
   "source": [
    "Check if the path exists, if not create it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a13159",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(corpus_path):\n",
    "    print(f'Creating path: {corpus_path}')\n",
    "    os.makedirs(corpus_path)\n",
    "else:\n",
    "    print(f'Path already exists: {corpus_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407c9cbc",
   "metadata": {},
   "source": [
    "Specify the CSV file name here and the field name you are using for prompting. You can also specify the encoding of the file. Leave encoding as it is for files encoded with UTF-8.  \n",
    "\n",
    "The example is from some recent RadioNZ stories about AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'sample-for-example3.csv'\n",
    "field_name = 'title'\n",
    "encoding = 'utf-8-sig'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0cce0",
   "metadata": {},
   "source": [
    "Read the data for field_name and preview it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(csv_file, 'r', encoding = encoding, newline = '') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    \n",
    "    header = csv_reader.fieldnames\n",
    "    print(f'Header fields: {header}')\n",
    "    if field_name not in header:\n",
    "        print(f'The field name {field_name} is not in the header row!')\n",
    "    else:\n",
    "        seed_data = []\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            seed_data.append(row['title'])\n",
    "            \n",
    "print(f'Data for prompting: {seed_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70655fbe",
   "metadata": {},
   "source": [
    "Now we will generate a tiny corpus of three documents based on these title field.  \n",
    "Note: we are specifying a system prompt, but there is no user prompt.  \n",
    "The prompt for each story is the title we have generated above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "Write a news story for Radio New Zealand based on the title provided. This should be written for a general audience. \n",
    "Do not include the title as part of the output.\n",
    "'''\n",
    "\n",
    "max_tokens = 2048\n",
    "\n",
    "temperature = 1.5 \n",
    "\n",
    "response_format = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa07f11",
   "metadata": {},
   "source": [
    "Run this to generate the corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in seed_data:\n",
    "    print(f'Generating text based on: {prompt}')\n",
    "    response = query_llm(prompt, model, system_prompt, max_tokens, response_format = response_format, temperature = temperature)\n",
    "    print(f'Text {i} preview: {response[0:200]} ...')\n",
    "\n",
    "    filename = slugify(prompt, max_length=25) + '.txt' # Note: this creates a nice filename from the title\n",
    "    with open(os.path.join(corpus_path, filename), 'w', encoding='utf8') as f:\n",
    "        print(f'Saving to {os.path.join(corpus_path, filename)}')\n",
    "        f.write(response)\n",
    "    \n",
    "    print('-----------------')\n",
    "    time.sleep(10) # always use a time delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf9e14",
   "metadata": {},
   "source": [
    "## Example 4 - Interactive chatbot-like example, generating text based on memory of previous generated text\n",
    "\n",
    "This example illustrates how to use previously generated text as context to generate an interactive chat exchange."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080bfa8b",
   "metadata": {},
   "source": [
    "Set the path to save the corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af908cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'example4-corpus/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed7f96",
   "metadata": {},
   "source": [
    "Check if the path exists, if not create it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(corpus_path):\n",
    "    print(f'Creating path: {corpus_path}')\n",
    "    os.makedirs(corpus_path)\n",
    "else:\n",
    "    print(f'Path already exists: {corpus_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430b47d",
   "metadata": {},
   "source": [
    "Below are the settings we will use to generate a tiny corpus of five documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5432b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_texts_to_generate = 5 # PLEASE LEAVE THIS FOR NOW!\n",
    "\n",
    "system_prompt = '''\n",
    "Generate exchanges in an imaginary panel discussion on the political consequences of artificial intelligence. \n",
    "The people involved in the discussion are:\n",
    "- Mr Big, Fortune 500 CEO\n",
    "- Ms X, Political Activist\n",
    "- Dr Y, Former-AI researcher turned tech bro podcaster\n",
    "- Ms Z, Journalist leading the discussion\n",
    "\n",
    "For each exchange in the panel discussion, choose a different person to speak and generate their response to the prompt based on the context \n",
    "provided. \n",
    "\n",
    "The generated text should have the person's name on the first line in all caps, then a new line, then their response. \n",
    "\n",
    "<PERSON NAME>: \n",
    "<person's response>\n",
    "\n",
    "Only one person should respond to a prompt.\n",
    "\n",
    "CONTEXT:\n",
    "'''\n",
    "\n",
    "prompt = '''\n",
    "MS Z: \n",
    "I've been following the debates, and it's clear that there are sharp divisions. There are those who see AI as an opportunity \n",
    "and others who see it as a threat. What are the political consequences of artificial intelligence?\n",
    "'''\n",
    "\n",
    "max_tokens = 10000\n",
    "\n",
    "temperature = 1.0\n",
    "\n",
    "response_format = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9022634a",
   "metadata": {},
   "source": [
    "Running the next cell queries the API and generates text. A preview of each generated text is displayed and the generated text is saved as a .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bff47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ''\n",
    "\n",
    "print(f'Initial question: {prompt[0:200]} ...')\n",
    "print('------')\n",
    "\n",
    "for i in range(number_of_texts_to_generate):\n",
    "    system_prompt_with_context = system_prompt + context\n",
    "    response = query_llm(prompt, model, system_prompt_with_context, max_tokens, response_format = response_format, temperature = temperature)\n",
    "    print(f'Text {i} preview: {response[0:200]} ...')\n",
    "    \n",
    "    probable_speaker = response.split('\\n')[0]\n",
    "    if probable_speaker.isupper():\n",
    "        speaker = probable_speaker\n",
    "    else:\n",
    "        speaker = 'UNKNOWN'\n",
    "\n",
    "    filename = f'{i:03}-{slugify(speaker, max_length = 25)}.txt'\n",
    "    with open(os.path.join(corpus_path, filename), 'w', encoding='utf8') as f:\n",
    "        print(f'Saving to {os.path.join(corpus_path, filename)}')\n",
    "        f.write(response)\n",
    "    \n",
    "    context += prompt + '\\n'\n",
    "    prompt = response\n",
    "    \n",
    "    print('------')\n",
    "        \n",
    "    time.sleep(10) # always leave a delay!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef741e8a",
   "metadata": {},
   "source": [
    "Inspect your txt files in the corpus path you specified above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77142411",
   "metadata": {},
   "source": [
    "## Example 5: generate structured data and save to a CSV file\n",
    "\n",
    "You may want to save some generated structured data from an LLM. You could save the JSON response direct, but if you want to save to a format you can open and edit in Excel, CSV is a helpful format.\n",
    "\n",
    "In this bonus example, I generate structured data for members of an imaginary focus group. Each 'voter' has a name, age, location where they live, a job title and a party they favour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd8c992",
   "metadata": {},
   "source": [
    "Set the filename you want to save your data to. You should probably leave the encoding as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f803362",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'example5.csv'\n",
    "encoding = 'utf-8-sig'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f344920",
   "metadata": {},
   "source": [
    "Here are the prompts and other settings used to generate the structured data.\n",
    "\n",
    "If you modify the prompt, make sure you are generating an array of data formatted in a similar way. The rest of this example relies on the generated JSON being an array of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "Always respond with JSON data.\n",
    "'''\n",
    "\n",
    "prompt = '''\n",
    "Generate a list of 3 New Zealand voters.\n",
    "Each voter has a name, age, location where they live, job title, and a political party they support. \n",
    "Valid parties are National, Labour, Green, ACT, New Zealand First, Other, and Undecided.\n",
    "JSON format: \n",
    "{\n",
    "    \"voters\": [\n",
    "        {\n",
    "            \"name\": \"\",\n",
    "            \"age\": \"\",\n",
    "            \"location\": \"\",\n",
    "            \"job_title\": \"\",\n",
    "            \"party\": \"\",\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "\n",
    "max_tokens = 4098\n",
    "\n",
    "temperature = 1.0 \n",
    "\n",
    "response_format = 'json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da38026",
   "metadata": {},
   "source": [
    "Query the API to generate the titles ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3596e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_llm(prompt, model, system_prompt, max_tokens, response_format = response_format, temperature = temperature)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e7aaa",
   "metadata": {},
   "source": [
    "If the next cell runs and outputs the expected data you have some valid JSON and your CSV file will be populated. If not, run the cell above again. Sometimes LLMs don't follow output instructions! If you were generating different structured data, you may have to modify this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10fa3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(csv_file, 'w', encoding = encoding, newline = '') as file:\n",
    "    try:\n",
    "        seed_data = json.loads(response)\n",
    "        for i, seed in enumerate(seed_data['voters']):\n",
    "            if i == 0:\n",
    "                fieldnames = seed.keys()\n",
    "                print(f'Field names: {fieldnames}')\n",
    "                csv_writer = csv.DictWriter(file, fieldnames = fieldnames)\n",
    "                csv_writer.writeheader()\n",
    "\n",
    "            csv_writer.writerow(seed)\n",
    "            print(seed)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print('Error decoding JSON. Try regenerating. A lower temperature value may help.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f37aed",
   "metadata": {},
   "source": [
    "Inspect your CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac375c8",
   "metadata": {},
   "source": [
    "## Delete your corpus (if you need to)\n",
    "\n",
    "If you made a mistake or want to remove your corpus files for some other reason, you can use this part of the notebook. \n",
    "\n",
    "First, make sure the `corpus_path` below matches the corpus files you want to delete! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5710a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'example1-corpus/' # set this to whatever path makes sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d4e215",
   "metadata": {},
   "source": [
    "### Warning: this will delete files!\n",
    "\n",
    "If you want to delete your corpus files, change `i_want_to_delete_my_files` from `False` to `True` and run the cell. \n",
    "\n",
    "Make sure you change it back again afterwards so you don't run it accidentally and delete your files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_want_to_delete_my_files = False\n",
    "\n",
    "if i_want_to_delete_my_files == True:\n",
    "    for filename in os.listdir(corpus_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(corpus_path, filename)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0615a",
   "metadata": {},
   "source": [
    "## Save a zip file of your corpus\n",
    "\n",
    "If you want to download your corpus, you can zip it and download a .zip file. Set the filename for the zip file using `corpus_file_name`. Make sure the `corpus_path` matches the location of the files you want to download.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file_name = 'example1-corpus.zip'\n",
    "corpus_path = 'example1-corpus/'\n",
    "\n",
    "shutil.make_archive(corpus_file_name[:-4], 'zip', corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105de5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
